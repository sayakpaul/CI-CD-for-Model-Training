{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sayakpaul/CI-CD-for-Model-Training/blob/main/cloud_build_tfx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j59AaHsxePSW"
   },
   "source": [
    "## References\n",
    "\n",
    "* https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_vertex_training\n",
    "* https://github.com/GoogleCloudPlatform/mlops-with-vertex-ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tR7ZgI4DCPz"
   },
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DDXrX_Ee359"
   },
   "outputs": [],
   "source": [
    "!gcloud init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oy0ymAXdXKA"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhp1xhz6B1u5"
   },
   "outputs": [],
   "source": [
    "GOOGLE_CLOUD_PROJECT = \"fast-ai-exploration\"\n",
    "GOOGLE_CLOUD_REGION = \"us-central1\"\n",
    "GCS_BUCKET_NAME = \"vertex-tfx-mlops\"\n",
    "\n",
    "PIPELINE_NAME = \"penguin-vertex-training\"\n",
    "DATA_ROOT = \"gs://{}/data/{}\".format(GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "MODULE_ROOT = \"gs://{}/pipeline_module/{}\".format(GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "if not (GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):\n",
    "    from absl import logging\n",
    "\n",
    "    logging.error(\"Please set all required parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yu5Ojm61fH-S"
   },
   "outputs": [],
   "source": [
    "!gsutil cp gs://download.tensorflow.org/data/palmer_penguins/penguins_processed.csv {DATA_ROOT}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkMnLHuGDGXW"
   },
   "source": [
    "## Training module for TFX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6L7hhCufOeD"
   },
   "outputs": [],
   "source": [
    "_trainer_module_file = 'penguin_trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjAFV2szfRT7"
   },
   "outputs": [],
   "source": [
    "%%writefile {_trainer_module_file}\n",
    "\n",
    "# Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple and\n",
    "# slightly modified run_fn() to add distribution_strategy.\n",
    "\n",
    "from typing import List\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "\n",
    "_FEATURE_KEYS = [\n",
    "    \"culmen_length_mm\",\n",
    "    \"culmen_depth_mm\",\n",
    "    \"flipper_length_mm\",\n",
    "    \"body_mass_g\",\n",
    "]\n",
    "_LABEL_KEY = \"species\"\n",
    "\n",
    "_TRAIN_BATCH_SIZE = 20\n",
    "_EVAL_BATCH_SIZE = 10\n",
    "\n",
    "# Since we're not generating or creating a schema, we will instead create\n",
    "# a feature spec.  Since there are a fairly small number of features this is\n",
    "# manageable for this dataset.\n",
    "_FEATURE_SPEC = {\n",
    "    **{\n",
    "        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n",
    "        for feature in _FEATURE_KEYS\n",
    "    },\n",
    "    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64),\n",
    "}\n",
    "\n",
    "\n",
    "def _input_fn(\n",
    "    file_pattern: List[str],\n",
    "    data_accessor: tfx.components.DataAccessor,\n",
    "    schema: schema_pb2.Schema,\n",
    "    batch_size: int,\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"Generates features and label for training.\n",
    "\n",
    "    Args:\n",
    "      file_pattern: List of paths or patterns of input tfrecord files.\n",
    "      data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "      schema: schema of the input data.\n",
    "      batch_size: representing the number of consecutive elements of returned\n",
    "        dataset to combine in a single batch\n",
    "\n",
    "    Returns:\n",
    "      A dataset that contains (features, indices) tuple where features is a\n",
    "        dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "    \"\"\"\n",
    "    return data_accessor.tf_dataset_factory(\n",
    "        file_pattern,\n",
    "        tfxio.TensorFlowDatasetOptions(batch_size=batch_size, label_key=_LABEL_KEY),\n",
    "        schema=schema,\n",
    "    ).repeat()\n",
    "\n",
    "\n",
    "def _make_keras_model(learning_rate: float) -> tf.keras.Model:\n",
    "    \"\"\"Creates a DNN Keras model for classifying penguin data.\n",
    "\n",
    "    Returns:\n",
    "      A Keras Model.\n",
    "    \"\"\"\n",
    "    # The model below is built with Functional API, please refer to\n",
    "    # https://www.tensorflow.org/guide/keras/overview for all API options.\n",
    "    inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n",
    "    d = keras.layers.concatenate(inputs)\n",
    "    for _ in range(2):\n",
    "        d = keras.layers.Dense(8, activation=\"relu\")(d)\n",
    "    outputs = keras.layers.Dense(3)(d)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    model.summary(print_fn=logging.info)\n",
    "    return model\n",
    "\n",
    "\n",
    "# NEW: Read `use_gpu` from the custom_config of the Trainer.\n",
    "#      if it uses GPU, enable MirroredStrategy.\n",
    "def _get_distribution_strategy(fn_args: tfx.components.FnArgs):\n",
    "    if fn_args.custom_config.get(\"use_gpu\", False):\n",
    "        logging.info(\"Using MirroredStrategy with one GPU.\")\n",
    "        return tf.distribute.MirroredStrategy(devices=[\"device:GPU:0\"])\n",
    "    return None\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "    \"\"\"Train the model based on given args.\n",
    "\n",
    "    Args:\n",
    "      fn_args: Holds args used to train the model as name/value pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    # This schema is usually either an output of SchemaGen or a manually-curated\n",
    "    # version provided by pipeline author. A schema can also derived from TFT\n",
    "    # graph if a Transform component is used. In the case when either is missing,\n",
    "    # `schema_from_feature_spec` could be used to generate schema from very simple\n",
    "    # feature_spec, but the schema returned would be very primitive.\n",
    "    schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n",
    "    hyperparameters = fn_args.hyperparameters\n",
    "    logging.info(\"Hyperparameters:\")\n",
    "    logging.info(hyperparameters)\n",
    "\n",
    "    train_dataset = _input_fn(\n",
    "        fn_args.train_files, fn_args.data_accessor, schema, batch_size=_TRAIN_BATCH_SIZE\n",
    "    )\n",
    "    eval_dataset = _input_fn(\n",
    "        fn_args.eval_files, fn_args.data_accessor, schema, batch_size=_EVAL_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # NEW: If we have a distribution strategy, build a model in a strategy scope.\n",
    "    strategy = _get_distribution_strategy(fn_args)\n",
    "    if strategy is None:\n",
    "        model = _make_keras_model(hyperparameters[\"learning_rate\"])\n",
    "    else:\n",
    "        with strategy.scope():\n",
    "            model = _make_keras_model(hyperparameters[\"learning_rate\"])\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        epochs=hyperparameters[\"num_epochs\"],\n",
    "    )\n",
    "\n",
    "    # The result of the training should be saved in `fn_args.serving_model_dir`\n",
    "    # directory.\n",
    "    model.save(fn_args.serving_model_dir, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Khip4KnDfWAw"
   },
   "outputs": [],
   "source": [
    "!gsutil cp {_trainer_module_file} {MODULE_ROOT}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6jgBaknDJaz"
   },
   "source": [
    "## Cloud Build configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqNAjXJO7aHH"
   },
   "outputs": [],
   "source": [
    "REPO_URL = \"https://github.com/sayakpaul/CI-CD-for-Model-Training\"\n",
    "BRANCH = \"dev\"\n",
    "\n",
    "PIPELINE_ROOT = \"gs://{}/pipeline_root/{}\".format(GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "SERVING_MODEL_DIR = \"gs://{}/serving_model/{}\".format(GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "VERSION = \"1.0.0\"\n",
    "CICD_IMAGE_URI = f\"gcr.io/tfx-oss-public/tfx:{VERSION}\"\n",
    "TFX_IMAGE_URI = f\"gcr.io/{GOOGLE_CLOUD_PROJECT}/{PIPELINE_NAME}:{VERSION}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZcjZcO87L3A"
   },
   "outputs": [],
   "source": [
    "SUBSTITUTIONS=f\"\"\"\\\n",
    "_REPO_URL='{REPO_URL}',\\\n",
    "_BRANCH={BRANCH},\\\n",
    "_PROJECT={GOOGLE_CLOUD_PROJECT},\\\n",
    "_REGION={GOOGLE_CLOUD_REGION},\\\n",
    "_PIPELINE_NAME={PIPELINE_NAME},\\\n",
    "_PIPELINE_ROOT={PIPELINE_ROOT},\\\n",
    "_MODULE_ROOT={MODULE_ROOT},\\\n",
    "_DATA_ROOT={DATA_ROOT},\\\n",
    "_SERVING_MODEL_DIR={SERVING_MODEL_DIR},\\\n",
    "_CICD_IMAGE_URI={CICD_IMAGE_URI},\\\n",
    "_TFX_IMAGE_URI={TFX_IMAGE_URI}\n",
    "\"\"\"\n",
    "\n",
    "!echo $SUBSTITUTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEjWZxeNDN41"
   },
   "source": [
    "## Submit to Cloud Build\n",
    "\n",
    "The output of Cloud Build, in this case, is a compiled pipeline uploaded to GCS Bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plY19wz89cK_"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/sayakpaul/CI-CD-for-Model-Training --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AldFybUB8P22"
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit --no-source --timeout=60m \\\n",
    "    --config CI-CD-for-Model-Training/build/pipeline-deployment.yaml \\\n",
    "    --substitutions {SUBSTITUTIONS} \\\n",
    "    --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hL3uyUknRSjX"
   },
   "source": [
    "Output:\n",
    "\n",
    "```shell\n",
    "ID                                    CREATE_TIME                DURATION  SOURCE  IMAGES  STATUS\n",
    "1619041e-a192-4de0-91f5-6799afa647ca  2021-08-24T08:16:37+00:00  7M45S     -       -       SUCCESS\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e4Gz8JSRYN0"
   },
   "outputs": [],
   "source": [
    "!gsutil ls -lh {PIPELINE_ROOT}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzATif02TA6u"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "The output of the build is a compiled pipeline specification file (`.json`) uploaded to the specified GCS Bucket. This file can be provided like so in order to start the execution on Vertex AI:\n",
    "\n",
    "```python\n",
    "from kfp.v2.google import client\n",
    "\n",
    "pipelines_client = client.AIPlatformClient(\n",
    "    project_id=GOOGLE_CLOUD_PROJECT,\n",
    "    region=GOOGLE_CLOUD_REGION,\n",
    ")\n",
    "\n",
    "_ = pipelines_client.create_run_from_job_spec(f\"{PIPELINE_ROOT}/{PIPELINE_NAME}.json\", \n",
    "    parameter_values={\n",
    "        'num_epochs': 3, 'learning_rate': 0.01\n",
    "    },\n",
    "    enable_caching=True)\n",
    "```\n",
    "\n",
    "However, we will use this pipeline slightly differently in order to allow ***continuous training***. For more details, follow [this notebook](https://colab.research.google.com/github/sayakpaul/CI-CD-for-Model-Training/blob/dev/cloud_function_trigger.ipynb)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "cloud_build_tfx.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
